{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "import bs4\n",
    "import yaml\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a method that reads YAML data from a file, parses it, and returns a Sites instance populated with data from the YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Dict\n",
    "\n",
    "class Site(BaseModel):\n",
    "    blocks: List[str]\n",
    "    links: List[str]\n",
    "\n",
    "class Sites(BaseModel):\n",
    "    sites: Dict[str, Site]\n",
    "\n",
    "    @classmethod\n",
    "    def from_yaml_file(cls, file_path: str) -> 'Sites':\n",
    "        \"\"\"\n",
    "        Class method to create a Sites instance from a YAML file.\n",
    "\n",
    "        :param file_path: Path to the YAML file.\n",
    "        :return: An instance of Sites populated with the YAML data from the file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "        return cls(**data)\n",
    "\n",
    "sites_instance = Sites.from_yaml_file('links.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up model and host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MODEL = \"llama3:8b\" # ollama model from library https://ollama.com/library\n",
    "OLLAMA_HOST = \"http://galactica.lan:11434\" # host that runs OLLAMA server app and serves the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If model unavalible on target host then download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = ollama.Client(host=OLLAMA_HOST)\n",
    "# client.pull(USE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=USE_MODEL, base_url=OLLAMA_HOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for site_name, site in sites_instance.sites.items():\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=site.links,\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(class_=site.blocks)\n",
    "        ),\n",
    "    )\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split it into 1,000-character chunks with 200-character overlaps using the RecursiveCharacterTextSplitter. This method helps maintain context and facilitates efficient retrieval. We'll also keep track of each chunk's starting index for reference.\n",
    "\n",
    "To search text chunks, we'll embed each one and store these embeddings in a vector database. When querying, we embed the search text and use cosine similarity to find chunks with similar embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OllamaEmbeddings(model=USE_MODEL, base_url=OLLAMA_HOST),\n",
    "    persist_directory=\"./chroma_storage\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert your vector store into a retriever with vectorstore.as_retriever(). This will allow you to search and retrieve relevant documents based on a query. \n",
    "hub.pull() to get a specific prompt from a repository. In this case, \"rlm/rag-prompt\" is the identifier for the prompt.\n",
    "Define a function format_docs() that takes a list of documents and concatenates their content into a single string, separated by double newlines. This formatted text will be used as context for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a RAG chain to process the input question, search and format relevant documents, apply the prompt, and generate the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What kind of skin care is best? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"When are oral medications considered?  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"Do topical medications help? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What are some examples of exposome factors?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What specific type of professionals comprised the board that held the meetings?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insightful-routines-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
